{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport seaborn as sns\nimport pandas_profiling\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport sklearn.metrics as metrics\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, roc_curve,auc, confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize NaN's\n\nmsno.matrix(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# inspect features\n\nprofile = pandas_profiling.ProfileReport(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new features: 'relatives' & 'travelled alone'\n\ndata = [train_df, test_df]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'travelled_alone'] = 'No'\n    dataset.loc[dataset['relatives'] == 0, 'travelled_alone'] = 'Yes'\n    \ntrain_df['travelled_alone'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop 'PassengerId', because it does not affect survival probability\n\ntrain_df = train_df.drop(['PassengerId'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Engineer new feature, 'Deck', from 'Cabin'\n\nimport re\ndeck = {\"A\": \"A\", \"B\": \"B\", \"C\": \"C\", \"D\": \"D\", \"E\": \"E\", \"F\": \"F\", \"G\": \"G\", \"U\": \"U\"}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(\"U\")\n\n# Drop cabin feature\ntrain_df = train_df.drop(['Cabin'], axis=1)\ntest_df = test_df.drop(['Cabin'], axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill NaN values in 'Age' with random values generated using mean and std dev\n\ndata = [train_df, test_df]\n\nfor dataset in data:\n    mean = train_df[\"Age\"].mean()\n    std = test_df[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_df[\"Age\"].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"common_value = 'S'\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Fare'] = train_df['Fare'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Fare'] = test_df['Fare'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Begin the process of extracting titles\n\ntrain_titles = train_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntype(train_titles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inspect value counts for title\n\ntrain_titles.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Engineer new feature 'title', and map to an integer\n\ndata = [train_df, test_df]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    #dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(\"NA\")\ntrain_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(['Ticket'], axis=1)\ntest_df = test_df.drop(['Ticket'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Engineer new feature, 'Age_Class'\n\ndata = [train_df, test_df]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age']* dataset['Pclass']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Engineer new feature, \"Fare_Per_Person\"\n\nfor dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign 'Age' to categories\n\ndata = [train_df, test_df]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 2, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 2) & (dataset['Age'] <= 12), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 12) & (dataset['Age'] <= 18), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 24), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 24) & (dataset['Age'] <= 45), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 45) & (dataset['Age'] <= 64), 'Age'] = 5\n   # dataset.loc[(dataset['Age'] > 55) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 6\n    \n    dataset['Age'] = dataset['Age'].astype(str)\n    dataset.loc[ dataset['Age'] == '0', 'Age'] = \"Children\"\n    dataset.loc[ dataset['Age'] == '1', 'Age'] = \"Teens\"\n    dataset.loc[ dataset['Age'] == '2', 'Age'] = \"Youngsters\"\n    dataset.loc[ dataset['Age'] == '3', 'Age'] = \"Young Adults\"\n    dataset.loc[ dataset['Age'] == '4', 'Age'] = \"Adults\"\n    dataset.loc[ dataset['Age'] == '5', 'Age'] = \"Middle Age\"\n    dataset.loc[ dataset['Age'] == '6', 'Age'] = \"Senior\"\n   # dataset.loc[ dataset['Age'] == '7', 'Age'] = \"Retired\"\n\n# inspect how age is distributed\ntrain_df['Age'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign 'Fare' to categories\n\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    dataset['Fare'] = dataset['Fare'].astype(str)\n    dataset.loc[ dataset['Fare'] == '0', 'Fare'] = \"Extremely Low\"\n    dataset.loc[ dataset['Fare'] == '1', 'Fare'] = \"Very Low\"\n    dataset.loc[ dataset['Fare'] == '2', 'Fare'] = \"Low\"\n    dataset.loc[ dataset['Fare'] == '3', 'Fare'] = \"High\"\n    dataset.loc[ dataset['Fare'] == '4', 'Fare'] = \"Very High\"\n    dataset.loc[ dataset['Fare'] == '5', 'Fare'] = \"Extremely High\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Pclass'] = dataset['Pclass'].astype(str)\n    dataset.loc[ dataset['Pclass'] == '1', 'Pclass'] = \"Class1\"\n    dataset.loc[ dataset['Pclass'] == '2', 'Pclass'] = \"Class2\"\n    dataset.loc[ dataset['Pclass'] == '3', 'Pclass'] = \"Class3\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Capture all the numerical features so they can be scaled\n\ntrain_numerical_features = list(train_df.select_dtypes(include=['int64', 'float64', 'int32']).columns)\ntrain_numerical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_numerical_features[0]\ntrain_numerical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature scaling\nss_scaler = StandardScaler()\ntrain_df_ss = pd.DataFrame(data = train_df)\ntrain_df_ss[train_numerical_features] = ss_scaler.fit_transform(train_df_ss[train_numerical_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_numerical_features = list(test_df.select_dtypes(include=['int64', 'float64', 'int32']).columns)\ntest_numerical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del test_numerical_features[0]\ntest_numerical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature scaling\n\ntest_ss_scaler = StandardScaler()\ntest_df_ss = pd.DataFrame(data = test_df)\ntest_df_ss[test_numerical_features] = test_ss_scaler.fit_transform(test_df_ss[test_numerical_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-Hot encoding\n\nencode_col_list = list(train_df.select_dtypes(include=['object']).columns)\nfor i in encode_col_list:\n    train_df_ss = pd.concat([train_df_ss,pd.get_dummies(train_df_ss[i], prefix=i)],axis=1)\n    train_df_ss.drop(i, axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-Hot encoding\n\ntest_encode_col_list = list(test_df.select_dtypes(include=['object']).columns)\nfor i in test_encode_col_list:\n    test_df_ss = pd.concat([test_df_ss,pd.get_dummies(test_df_ss[i], prefix=i)],axis=1)\n    test_df_ss.drop(i, axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_df_ss.drop(\"Survived\", axis=1)\ny_train = train_df_ss[\"Survived\"]\nX_test  = test_df_ss.drop(\"PassengerId\", axis=1).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features\nX = X_train \n\n# Target variable\ny = y_train ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) #, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classify using a Random Forrest (RF)\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nRF = RandomForestClassifier(n_estimators=100)\nRF.fit(X_train,y_train)\n\ny_pred=RF.predict(X_train)\nRF_acc = metrics.accuracy_score(y_train, y_pred)\n\nprint(\"Accuracy:\",RF_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assess RF accuracy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classify using Naive Bayes (NB)\n\nfrom sklearn.naive_bayes import GaussianNB\n\nGNB = GaussianNB()\nGNB.fit(X_train, y_train)\ny_pred = GNB.predict(X_train)\nGNB_acc = metrics.accuracy_score(y_train, y_pred)\n\nprint(\"Accuracy:\",GNB_acc)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classify using Logisitic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nLogReg = LogisticRegression()\nLogReg.fit(X_train,y_train)\n\ny_pred=LogReg.predict(X_train)\nLogReg_acc = metrics.accuracy_score(y_train, y_pred)\n\nprint(\"Accuracy:\",LogReg_acc)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nLDA = LDA(n_components=1)\nLDA.fit(X_train, y_train)\n\ny_pred=LDA.predict(X_train)\nLDA_acc = metrics.accuracy_score(y_train, y_pred)\n\nprint(\"Accuracy:\",LDA_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classify using XG Boost (XGB)\nfrom xgboost import XGBClassifier\n\nXGB = XGBClassifier()\nXGB.fit(X_train, y_train)\n\ny_pred = XGB.predict(X_train)\nXGB_acc = metrics.accuracy_score(y_train, y_pred)\n\nprint(\"Accuracy:\", XGB_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Tune XGB paramters using brute force\n\n#import warnings\n#warnings.filterwarnings('ignore')\n\n#from datetime import datetime\n#from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n#from sklearn.metrics import roc_auc_score\n#from sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Time our brute force parameter tuning\n\n#    if not start_time:\n#        start_time = datetime.now()\n#        return start_time\n#    elif start_time:\n#        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n#        tmin, tsec = divmod(temp_sec, 60)\n#        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# A parameter grid for XGBoost (bruteforce)\n\n#params = {\n#        'min_child_weight': [1, 5, 10],\n#        'gamma': [0.5, 1, 1.5, 2, 5],\n#        'subsample': [0.6, 0.8, 1.0],\n#        'colsample_bytree': [0.6, 0.8, 1.0],\n#        'max_depth': [3, 4, 5]\n #       }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n#                    silent=True, nthread=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#folds = 5\n#param_comb = 5\n\n#skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1)\n\n#random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,y), verbose=3, random_state=1 )\n\n\n#start_time = timer(None)\n#random_search.fit(X, y)\n#timer(start_time) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#print('\\n All results:')\n#print(random_search.cv_results_)\n#print('\\n Best estimator:')\n#print(random_search.best_estimator_)\n#print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n#print(random_search.best_score_ * 2 - 1)\n#print('\\n Best hyperparameters:')\n#print(random_search.best_params_)\n#results = pd.DataFrame(random_search.cv_results_)\n#results.to_csv('xgb-random-grid-search-results-01.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#XGB = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#              colsample_bynode=1, colsample_bytree=1.0, gamma=1, gpu_id=-1,\n#              importance_type='gain', interaction_constraints='',\n#              learning_rate=0.02, max_delta_step=0, max_depth=5,\n#              min_child_weight=5, monotone_constraints='()',\n#              n_estimators=600, n_jobs=1, nthread=1, num_parallel_tree=1,\n#              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n#              silent=True, subsample=1.0, tree_method='exact',\n#              validate_parameters=1, verbosity=None)\n\n#XGB.fit(X_train, y_train)\n#y_pred = XGB.predict(X_train)\n#XGB_acc = metrics.accuracy_score(y_train, y_pred)\n#print(\"Accuracy:\",XGB_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', \n              'Random Forest', 'Naive Bayes','XGBoost', 'LDA'],\n    'Score': [LogReg_acc, RF_acc, GNB_acc, \n              XGB_acc, LDA_acc]})\nmodels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_preds = RF.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": submission_preds\n    })\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#        TO \n#                BE \n#                        CONTINUED......","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}